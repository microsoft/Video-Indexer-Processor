{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest MSR-VTT Dataset\n",
    "In this notebook, we are converting metadata extracted from the MSR-VTT dataset into a newml suitable format. The videos and metadata can be found in the following location\n",
    "https://github.com/crux82/msr-vtt-it\n",
    "\n",
    "#### How to download the video files?\n",
    "Video files were download manually to a local computer from this [url](https://github.com/crux82/msr-vtt-it/tree/master/msr-vtt-it) and then uploaded to an Azure Blob Storage \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the JSON file - containing all the annotations for the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (c) Microsoft Corporation.\n",
    "Licensed under the MIT license.\n",
    "\"\"\"\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('./common/notebooks/dataset_ingestion/msr_vtt/msr_vtt_raw_data/train_val_videodatainfo.json')\n",
    "# returns JSON object as a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Category File - converts annotated category id into a string representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "category = open('./common/notebooks/dataset_ingestion/msr_vtt/msr_vtt_raw_data/category.txt').read()\n",
    "category_df = pd.DataFrame([x.split('\\t') for x in category.split('\\n')])\n",
    "category_df.columns = ['categ_name','categ_id']\n",
    "category_df['categ_id'] = category_df['categ_id'].astype(str)\n",
    "category_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract video information and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = data['videos']\n",
    "metadata = data['sentences']\n",
    "df = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = pd.DataFrame(videos)\n",
    "videos['category'] = videos['category'].astype('str')\n",
    "videos = videos.merge(category_df, left_on = 'category',right_on='categ_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the annotation file is missing video creation dates, we will fake them using the last 2 year period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "\n",
    "# fake creation dates to avoid going to Youtube API\n",
    "def random_date(start, end):\n",
    "    \"\"\"Generate a random datetime between `start` and `end`\"\"\"\n",
    "    return start + datetime.timedelta(\n",
    "        # Get a random amount of seconds between `start` and `end`\n",
    "        seconds=random.randint(0, int((end - start).total_seconds())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all annotation for a single video as a long sentence\n",
    "full_video_description = df.groupby('video_id')['caption'].apply('. '.join).reset_index()\n",
    "# videos contain multiple languages, so we will use auto to detect languages\n",
    "full_video_description['video_languages'] = \"Various Languages\"\n",
    "full_video_description['video_languages_code'] = 'auto'\n",
    "# Now let's fake the creation date, each video will have only 1 version so creation date == current version date\n",
    "full_video_description['first_creation_date'] = full_video_description['video_id'].apply(lambda x: random_date(pd.to_datetime('2020-01-01'),pd.to_datetime('2022-07-20')))\n",
    "full_video_description['current_version_creation_date'] = full_video_description['first_creation_date']\n",
    "# No restrictions on usage terms\n",
    "full_video_description['usage_terms'] = 'Mictosoft Research - no restrictions'\n",
    "\n",
    "full_video_description['matching_video_name'] = full_video_description['video_id'].apply(lambda x: x+'.mp4')\n",
    "full_video_description['data_source'] = 'msr-vtt'\n",
    "full_video_description['video_description'] = full_video_description['caption']\n",
    "full_video_description['version'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all the information together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full_video_description.merge(videos, left_on = 'video_id', right_on='video_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['first_creation_date','current_version_creation_date','video_id','video_description','video_languages','usage_terms','matching_video_name','video_languages_code','data_source','url','categ_name']]\n",
    "data = data.rename(columns = {'categ_name':'topics'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Categories into a list of topics (requirement of MetadataParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the youtube url \n",
    "data['video_description'] = data.apply(lambda row: 'video shows: ' + row['video_description'] +'. video_url:' + row['url'], axis=1)\n",
    "\n",
    "data['keywords'] = data['topics'].apply(lambda x: x.split('/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all videos showing children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['video_description'].str.contains('child|kid|boy|girl', na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DataFrame into a list of dictionaries and save them as JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['first_creation_date'] = data['first_creation_date'].astype(str)\n",
    "data['current_version_creation_date'] = data['current_version_creation_date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.enrichment.xml_parser.metadata_parser import MetadataParser\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# setting up variables for connection and source to the source storage acount/container\n",
    "source_account_key = os.getenv('SOURCE_ACCOUNT_STORAGE_KEY')\n",
    "storage_account = os.getenv('SOURCE_ACCOUNT_STORAGE_NAME')\n",
    "container_name = 'msr-vtt'\n",
    "source_connection_string = f'DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={source_account_key};EndpointSuffix=core.windows.net'\n",
    "source_service = BlobServiceClient.from_connection_string(conn_str=source_connection_string)\n",
    "\n",
    "\n",
    "# setting up variables for connection and source to the source storage acount/container\n",
    "target_account_key = os.getenv('WALDO_UPLOAD_STORAGE_KEY')\n",
    "target_storage_account = os.getenv('WALDO_STORAGE_ACCOUNT_NAME')\n",
    "target_container_name = os.getenv('WALDO_CONTAINER_NAME')\n",
    "target_connection_string = f'DefaultEndpointsProtocol=https;AccountName={target_storage_account};AccountKey={target_account_key};EndpointSuffix=core.windows.net'\n",
    "target_service = BlobServiceClient.from_connection_string(conn_str=target_connection_string)\n",
    "\n",
    "source_container_client = source_service.get_container_client(container_name)\n",
    "target_container_client = target_service.get_container_client(target_container_name)\n",
    "bronze_container_client = target_service.get_container_client('bronze')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "records = data.to_dict('records')\n",
    "source = 'msr-vtt'\n",
    "for record in records:\n",
    "\n",
    "    # parse metadata file \n",
    "    parser  = MetadataParser()\n",
    "    parsed = parser.parse_metadata(record)\n",
    "\n",
    "    # upload metadata file to directory in upload\n",
    "    new_blob_name = f\"{source}/{parsed['file_name']}/{parsed['file_name']}.json\"\n",
    "    print(new_blob_name)\n",
    "    blob_client = target_container_client.get_blob_client(new_blob_name)\n",
    "    try:\n",
    "        blob_client.upload_blob(json.dumps(parsed), overwrite=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    \n",
    "    # Move the video file to a new location\n",
    "    video_blob  =  bronze_container_client.get_blob_client(f\"MP4/{parsed['matching_video_name']}\")\n",
    "    print(video_blob.url)\n",
    "    target_blob_name = f\"{source}/{parsed['file_name']}/{parsed['matching_video_name']}\"\n",
    "    target_video_blob =  target_container_client.get_blob_client(target_blob_name) \n",
    "    try:\n",
    "        target_video_blob.start_copy_from_url(video_blob.url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c10a2a6bb51805b3f2c7da176b506728fcd4842ded6c1f89423002188fd8bedf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
